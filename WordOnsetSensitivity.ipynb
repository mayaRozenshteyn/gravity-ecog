{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02b326c-79f8-4827-90f8-4689b38085b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import csv\n",
    "import librosa\n",
    "import random\n",
    "from scipy.stats import rankdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "059d655a-12bc-4c4f-be74-b786aadbbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_eleccount = # Removed to protect patient anonymity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5225a238-493d-4b0b-8475-e4c81108edc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generating Word Onset Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91483c9-2661-4d8d-b611-f304e1bb2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_transcript_by_movie(patient_id, ecog1_t, ecog2_t):\n",
    "    # Load in interview transcript\n",
    "    with open('/scratch/gpfs/mayaar/GravityECoG/sourcedata/interview-transcripts/cleaned/cleaned_transcript_ny' + patient_id + '.csv', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        \n",
    "    # Movie 1\n",
    "    onsets_1 = []\n",
    "    # Movie 2\n",
    "    onsets_2 = []\n",
    "\n",
    "    # Check if word recorded during first or second ECoG recording\n",
    "    for i in range(len(data)):\n",
    "        if ((float(data[i][2]) > ecog1_t[0]) and (float(data[i][2]) < ecog1_t[-1])):\n",
    "            onsets_1.append(data[i][2])\n",
    "        if ((float(data[i][2]) > ecog2_t[0]) and (float(data[i][2]) < ecog2_t[-1])):\n",
    "            onsets_2.append(data[i][2])\n",
    "\n",
    "    onsets_1 = np.array(onsets_1, dtype=float)\n",
    "    onsets_2 = np.array(onsets_2, dtype=float)\n",
    "    \n",
    "    return onsets_1, onsets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa369d4-1070-4804-9283-9fb2c162f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rate = 512 #Hz\n",
    "downsampled_rate = 100 #Hz\n",
    "\n",
    "def downsample_ecog(ecog):\n",
    "    ecog_downsampled = librosa.resample(ecog, orig_sr=original_rate, target_sr=downsampled_rate)\n",
    "    return ecog_downsampled   \n",
    "\n",
    "def save_downsampled_ecog(patient_id, ecog1_t, ecog2_t):    \n",
    "    # Downsample EcoG to 100 Hz\n",
    "    ecog1 = np.load(\"../../sourcedata/non-ranked-ecog/nr-ecog1-\" + patient_id + \".npy\", allow_pickle = True)\n",
    "    ecog2 = np.load(\"../../sourcedata/non-ranked-ecog/nr-ecog2-\" + patient_id + \".npy\", allow_pickle = True)\n",
    "    ds_ecog1 = np.apply_along_axis(downsample_ecog, 1, ecog1)\n",
    "    ds_ecog2 = np.apply_along_axis(downsample_ecog, 1, ecog2)\n",
    "    \n",
    "    ds_ecog1_t = np.linspace(ecog1_t[0], ecog1_t[-1], ds_ecog1.shape[1])\n",
    "    ds_ecog2_t = np.linspace(ecog2_t[0], ecog2_t[-1], ds_ecog2.shape[1])\n",
    "    \n",
    "    # Save dowsampled ECog + Time Axis\n",
    "    np.save(\"ds-ecog1-\" + patient_id + \".npy\", ds_ecog1)\n",
    "    np.save(\"ds-ecog1-\" + patient_id + \"-t.npy\", ds_ecog1_t)\n",
    "    np.save(\"ds-ecog2-\" + patient_id + \".npy\", ds_ecog2)\n",
    "    np.save(\"ds-ecog2-\" + patient_id + \"-t.npy\", ds_ecog2_t)\n",
    "    \n",
    "    # Tie Ranking Downsampled ECoG\n",
    "    tie_ranked_ecog1 = rankdata(ds_ecog1, axis=1)\n",
    "    tie_ranked_ecog2 = rankdata(ds_ecog2, axis=1)\n",
    "    np.save(\"tr-ds-ecog1-\" + patient_id + \".npy\", tie_ranked_ecog1)\n",
    "    np.save(\"tr-ds-ecog2-\" + patient_id + \".npy\", tie_ranked_ecog2)\n",
    "    \n",
    "    return ds_ecog1_t, ds_ecog2_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcad4aa0-c64f-411c-bc8c-f93c3b7e7a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ecog_index_of_onset(word_onset, ecog_time_axis):\n",
    "    # calculate the difference array\n",
    "    difference_array = np.absolute(ecog_time_axis-word_onset)\n",
    "\n",
    "    # find the index of minimum element from the array\n",
    "    index = difference_array.argmin()\n",
    "    \n",
    "    return index\n",
    "\n",
    "def get_word_onset_markers(patient_id, onsets_1, onsets_2, ds_ecog1_t, ds_ecog2_t):\n",
    "    word_onsets_1 = np.zeros(ds_ecog1_t.shape[0])\n",
    "    for i in range(len(onsets_1)):\n",
    "        word_onsets_1[get_ecog_index_of_onset(onsets_1[i], ds_ecog1_t)] = 1\n",
    "        \n",
    "    word_onsets_2 = np.zeros(ds_ecog2_t.shape[0])\n",
    "    for i in range(len(onsets_2)):\n",
    "        word_onsets_2[get_ecog_index_of_onset(onsets_2[i], ds_ecog2_t)] = 1\n",
    "    \n",
    "    np.save(\"ecog1-\" + patient_id + \"-onset-marker.npy\", word_onsets_1)\n",
    "    np.save(\"ecog2-\" + patient_id + \"-onset-marker.npy\", word_onsets_2)\n",
    "    \n",
    "    return word_onsets_1, word_onsets_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a673b7-5f53-46d6-99ac-171ebc2a44ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_marker_shuffle(word_onsets):\n",
    "    # Step 1: Replace each element in onset marker array with number of 1s seen \n",
    "    # thusfar up to the current element\n",
    "    cumm_sum = np.zeros(len(word_onsets))\n",
    "    j = 0\n",
    "    for i in range(len(word_onsets)):\n",
    "        if word_onsets[i] == 1:\n",
    "            j += 1\n",
    "        cumm_sum[i] = j\n",
    "\n",
    "    # Step 2: Group cummulative sum array based on number of neighboring identical elements\n",
    "    counts = []\n",
    "    counter = 0\n",
    "    for i in range(len(cumm_sum) - 1):\n",
    "        counter += 1\n",
    "        if cumm_sum[i] != cumm_sum[i + 1] :\n",
    "            counts.append(counter)\n",
    "            counter = 0\n",
    "\n",
    "    # Step 3: Randomly shuffle neighboring identical element counts\n",
    "    random.shuffle(counts)\n",
    "\n",
    "    # Step 4: Re-expand shuffled array into vector of 0s and 1s\n",
    "    accum_counts = np.add.accumulate(counts)\n",
    "    expanded_arr = np.zeros(len(word_onsets))\n",
    "    for i in range(len(accum_counts)):\n",
    "        expanded_arr[accum_counts[i]] = 1\n",
    "\n",
    "    return expanded_arr\n",
    "\n",
    "def shuffle_onset_markers(word_onsets_1, word_onsets_2):\n",
    "    shuff_markers_1 = np.zeros((1000, len(word_onsets_1)))\n",
    "    shuff_markers_2 = np.zeros((1000, len(word_onsets_2)))\n",
    "\n",
    "    for i in range(1000):\n",
    "        shuff_markers_1[i, :] = robust_marker_shuffle(word_onsets_1)\n",
    "        shuff_markers_2[i, :] = robust_marker_shuffle(word_onsets_2)\n",
    "        \n",
    "    np.save(\"ecog1-\" + patient_id + \"-shuffled-markers.npy\", shuff_markers_1)\n",
    "    np.save(\"ecog2-\" + patient_id + \"-shuffled-markers.npy\", shuff_markers_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c288a53-9b3d-4d37-a002-a240431f3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in id_eleccount:\n",
    "    # Load in ECoG time axis\n",
    "    ecog1_t = scipy.io.loadmat('/scratch/gpfs/mayaar/GravityECoG/derivatives/preprocessing/sub-ny' + patient_id + '/eeg1_manualica_notch_time.mat')['trial'][0]\n",
    "    ecog1_t = np.array(ecog1_t, dtype=float)\n",
    "    ecog2_t = scipy.io.loadmat('/scratch/gpfs/mayaar/GravityECoG/derivatives/preprocessing/sub-ny' + patient_id + '/eeg2_manualica_notch_time.mat')['trial'][0]\n",
    "    ecog2_t = np.array(ecog2_t, dtype=float)\n",
    "    \n",
    "    # Separate word onsets into corresponding ECoG recordings (1 and 2)\n",
    "    onsets_1, onsets_2 = segment_transcript_by_movie(patient_id, ecog1_t, ecog2_t)\n",
    "    \n",
    "    # Downsample EcoG to 100 Hz + Tie Rank\n",
    "    ds_ecog1_t, ds_ecog2_t = save_downsampled_ecog(patient_id, ecog1_t, ecog2_t)\n",
    "    \n",
    "    # Create a long vector of zeros (length of the time axis) and set it to one at word onsets\n",
    "    word_onsets_1, word_onsets_2 = get_word_onset_markers(patient_id, onsets_1, onsets_2, ds_ecog1_t, ds_ecog2_t)\n",
    "    \n",
    "    # Shuffle Onset Markers 1000 times\n",
    "    shuffle_onset_markers(word_onsets_1, word_onsets_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91ef57-fe9e-4f48-a3bd-fca28416a068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grav-analysis [~/.conda/envs/grav-analysis/]",
   "language": "python",
   "name": "conda_grav-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
